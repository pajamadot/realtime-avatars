ntroduction

Interactive digital humans – realistic avatars that can respond in near real-time to user input – are becoming central to virtual communication, gaming, and AI assistants. Achieving a convincing “digital human” requires balancing visual realism, low latency, precise controllability, and feasible deployment. Recent advances (2023–2024) have produced several distinct approaches to real-time responsive avatars:

Graphics/Rigged Avatars: Using game-engine characters (e.g. Epic Games’ MetaHumans) driven by performance capture or animation rigs for real-time rendering.

Generative Video Models: Employing AI models (diffusion or transformer-based) that directly synthesize avatar video frames from audio or other signals.

Neural 3D Avatars (Gaussian Splatting): Representing a person as a neural 3D scene (e.g. 3D Gaussian splats) that can be efficiently animated and rendered in real-time.

Each approach comes with trade-offs in latency, fidelity, control, and system cost. In this survey, we review recent representative work in each category, compare their technical characteristics, and discuss emerging hybrid strategies that seek to combine the best of multiple methods. We focus on literature from 2023–2024 (with a few early 2025 advances) and provide references to papers and open-source projects.

MetaHuman-Based Real-Time Pipelines (Graphics and Traditional Animation)

Epic Games’ MetaHuman framework exemplifies the graphics-based approach to digital humans. MetaHumans are highly detailed 3D character models with rigged faces and bodies, designed for real-time rendering in Unreal Engine. By driving these rigs with input data (live video, motion capture, or audio), one can achieve interactive animation. In 2023, Epic introduced MetaHuman Animator, a tool that uses a 4D facial solver to combine video and depth data from a single camera (e.g. an iPhone) and automatically retarget an actor’s performance onto any MetaHuman character. This process produces high-fidelity facial animation reproducing the actor’s nuances and expressions on the digital avatar, with results ready in minutes on a local GPU. The system captures subtle details (down to tongue movement driven by audio cues) and outputs animation through the MetaHuman facial rig, ensuring that the generated facial motion is semantically valid (using the rig’s control curves) and temporally smooth.

Real-Time Control: Beyond offline processing, the MetaHuman pipeline supports true real-time streaming via Unreal’s Live Link system. As of MetaHuman 5.7, creators can animate a MetaHuman live from a camera or even just an audio input. For example, using the Live Link Face app on an iPhone (which streams facial blendshape parameters) or a simple microphone input, the avatar’s face can move and lip-sync in sync with the user’s speech. This approach leverages the optimized graphics engine for latency on the order of milliseconds – it’s feasible to achieve 60+ FPS animation since the heavy lifting (rendering the avatar) is handled by the GPU-accelerated Unreal Engine. The main latency is the capture and network transmission of facial data (typically <50 ms for local setups) and the engine’s processing. The controllability is very high: developers or animators can fine-tune expressions via the rig, blend in predefined animations, or adjust the output in real-time. Realism depends on the quality of the 3D model and rendering – MetaHumans are among the most realistic real-time characters available, with shaders for skin, teeth, eyes, etc., but they still have a CGI look compared to actual video. Achieving true photorealism can require careful lighting and perhaps post-processing. Cost and requirements: This approach needs substantial content creation effort upfront (designing or scanning the character) and a capable GPU to run the game engine. However, no example-specific AI training is required for each new character – one can create a new avatar by customizing a MetaHuman (or using scans via “Mesh to MetaHuman”) without collecting a large dataset, which is a strength of this pipeline. Tools like NVIDIA Omniverse Audio2Face similarly allow driving a 3D face (including MetaHumans) from audio using a pretrained ML model for the lipsync, combining traditional animation rigs with learned speech motion mapping.

Recent Innovations: The MetaHuman Animator (2023) demonstrates how traditional pipelines are adopting machine-learning-powered solvers to improve fidelity. It uses computer vision and GPU-based solving to extract facial motion from video in near real-time, dramatically reducing the manual effort previously needed to animate realistic faces. The result is an animation that can be applied to any character that shares the MetaHuman rig, allowing rapid transfer of performances. This showcases a hybrid of sorts: graphics-based real-time animation augmented by ML for performance capture. Nonetheless, the actual runtime puppet can still be purely rig-driven, which is highly efficient.

In summary, graphics-based avatar pipelines like MetaHuman offer low latency (potentially 30–60 FPS), precise control (via rigs/blendshapes), and integration with game engines for interactivity. They require significant content creation and their realism is bounded by the quality of 3D models and rendering – excellent, but often slightly short of photoreal. They shine in applications where full control and consistency are paramount (e.g. a virtual presenter that must reliably hit specific expressions or align perfectly with external motion capture).

Real-Time Generative Video Models (Diffusion & Transformers)

A very different approach is to use AI generative models to directly synthesize video frames of a talking or moving person. These models, often based on diffusion or transformer architectures, have recently achieved remarkable realism – to the point where a single input image of a person can be turned into a lifelike talking video of that person speaking new audio, with one-shot generalization to unseen identities. Generative avatar models typically take inputs like an identity image and a driving audio (and possibly text or other cues) and output a sequence of video frames. The key advantage is photorealism and minimal artist effort: because they learn from large datasets of real videos, they can produce outputs that look like real recorded footage, complete with natural head movements, eye blinks, etc., without requiring a manually crafted 3D asset.

State of the Art (2023–2024): Diffusion models have become the leading method for high-quality avatar video generation. However, vanilla diffusion is slow and not inherently suited for real-time, since it requires many iterative denoising steps and often processes an entire video clip at once (which introduces latency and prevents streaming). Recent research has attacked this problem from multiple angles:

Autoregressive streaming: Models like CausVid (2023) and later improvements (Self-Forcing, Seaweed, etc.) distill the heavy diffusion process into a much faster autoregressive predictor that generates a few frames at a time causally. By using techniques like block-wise causal attention (only looking at past frames, not future) and aggressive diffusion distillation, these works achieve orders-of-magnitude speedup. For example, Chern et al. (2025) introduce a real-time diffusion avatar “LiveTalk” that runs the denoising process at 1/40 of the original runtime via a two-stage distillation and refinement strategy. The result, called StreamAvatar in one incarnation, can produce streaming audio-driven video in real time, even maintaining a responsive “listening” state when the user is speaking. Ki et al. (2026) similarly propose Avatar Forcing, a framework combining diffusion forcing and a preference optimizer to attain <500 ms latency for interactive conversation head avatars. These advances ensure the model doesn’t need future context and can react immediately to live input.

Long-term consistency: A known challenge for autoregressive video generation is drift over long durations. Without careful design, the avatar’s identity can subtly change or quality can degrade after many frames. Researchers have introduced solutions such as Reference conditions or “identity anchors” (e.g. periodically feeding the original reference image features back in) and specialized attention mechanisms. StreamAvatar incorporates a Reference Sink to keep the model anchored on the person’s identity and a Reference-Anchored Positional Re-encoding (RAPR) to prevent positional drift in long sequences. These ensure that a 5-minute continuous generation of a talking head doesn’t veer off in appearance.

Quality vs speed trade-off: Even distilled models often sacrifice some visual fidelity. To combat this, a second-stage adversarial refinement can be applied. For example, StreamAvatar uses a consistency-aware discriminator to refine the student model’s outputs, improving sharpness and temporal coherence. This recovers detail lost in distillation, yielding videos that approach the quality of heavy non-real-time models.

Controllability: Generative models are typically driven by high-level controls – most commonly, an audio waveform for lip-sync and perhaps text to guide style/emotion. Some frameworks allow additional inputs: LiveTalk (Chern et al., 2025) conditions the diffusion on not just audio but also a “motion prompt” derived from a large language model analyzing conversation context. Others incorporate head pose or facial emotion codes if provided. However, fine-grained control (e.g. exact eyebrow raise timing) is generally not straightforward – the model decides many visual nuances. This is a trade-off: generative avatars excel at learned realism (e.g. naturally shaking the head, blinking, adding appropriate pauses) but may not hit exact predetermined actions unless explicitly conditioned. That said, some works integrate explicit controls: Lin et al. (2024) present GLDiTalker, which generates 3D facial motion (mesh sequences) from audio using a latent diffusion model over graph-based face representations. By generating a deformable 3D mesh, it effectively provides a controllable output (the mesh can be further adjusted or rendered as desired). GLDiTalker achieves about 30 FPS for driving a 3D face model on a single NVIDIA V100 GPU – a notable real-time milestone for diffusion-driven animation.

Latency and Performance: Recent systems have demonstrated impressively low latency given the complexity of generative models. For instance, LiveTalk (a 1.3B parameter diffusion) can respond with the first video frame in ~0.33 s and then sustain ~24–25 FPS generation thereafter on a single high-end GPU. A scaled-up version (SoulX-LiveTalk with a 14B model) reached ~32 FPS using 8× NVIDIA H800 GPUs in parallel – essentially real-time even at very high fidelity (this underscores that cloud deployment with multiple GPUs can overcome latency at the expense of cost). Academic projects report latencies around 0.5–1.5 s for start-up and sub-second per second of video, which is a massive improvement from older talking head models that often needed several seconds or were strictly offline. The use of block-wise generation and pipeline parallelism (where one GPU can render frames while another denoises the next block) has been key to achieving continuous playback.

Visual Realism: This approach currently leads in pure fidelity. Diffusion avatars produce photorealistic outputs indistinguishable from real video in many cases – including realistic hair, eye motions, and lighting consistent with the source image. They have largely overcome earlier “uncanny valley” issues in mouth movement; for example, one model reports state-of-the-art lip-sync scores (Sync-D) while maintaining image quality on par with top non-real-time models. However, realism can falter if the model encounters out-of-distribution inputs (e.g. an extreme audio speaking style or a face pose it never saw in training). Also, subtle artifacts or flicker can occasionally occur, especially if pushing for speed (fewer diffusion steps). But overall, diffusion-based avatars (e.g. OmniAvatar (Wan et al., 2024), Make-A-TalkingFace, etc.) now define the cutting edge for one-shot talking head generation with high fidelity. They are being quickly adopted in industry (many commercial avatar video services run variants of these models under the hood).

Cost and Data: Generative models require heavy upfront training on large video datasets. For instance, training a diffusion transformer for avatars might involve hundreds of hours of video of diverse speakers, and substantial compute (often done by big labs/companies). The benefit is one model generalizes to many faces (no per-person training needed). At runtime, the cost is running a neural network with millions/billions of parameters – typically needing a GPU. Real-time deployment might require a powerful single GPU (like an Nvidia A100 or better) or multiple GPUs working in parallel for larger models. This makes cloud-based deployment common for such systems (e.g. an API that takes in audio and an image and streams back video frames). In contrast to graphics pipelines, the per-instance cost (computation per each avatar stream) can be significant, though optimizations (quantization, distillation) are improving efficiency. Memory is another factor: maintaining identity consistency may require keeping certain keyframe embeddings or caches in memory for the duration of a session.

In summary, real-time generative video avatars have rapidly evolved to offer photorealism and one-shot convenience, at the cost of heavy compute and somewhat black-box control. They are ideal when a user can simply supply a photo and voice and get a lifelike digital double, without 3D modeling. Ongoing research is actively closing the latency and stability gaps to make these truly interactive (e.g. expressive reactions while listening, continuous minutes-long conversations without quality loss).

Streaming Neural Gaussian Avatars (3D Gaussian Splatting)

A third category bridges ideas from neural rendering and 3D graphics. Neural radiance fields (NeRFs) revolutionized view-synthesis, but vanilla NeRF was too slow for real-time and required per-scene training. In 2023, an alternative representation called 3D Gaussian Splatting (3DGS) emerged, enabling real-time rendering of photorealistic 3D scenes by using a cloud of Gaussian primitives instead of dense neural networks. This has paved the way for neural avatars: one can capture a real person as a collection of textured 3D Gaussians that can be quickly transformed and rendered from any viewpoint. By adding mechanisms to animate those Gaussians (either by learned deformation or by conditioning on driver signals), we get a streaming neural avatar that runs extremely fast and looks realistic because it’s essentially a volumetric replay of the person.

How it works: Typically, a multi-view video capture of a person (doing various expressions or motions) is used to optimize a Gaussian splatting model of that person. The result is somewhat like a personalized neural character – a digital asset that “looks exactly like X.” Once this model is trained, it can be driven either by a parametric rig or by an audio/motion model to produce new movements. For example, Zielonka et al. (2025) present D3GA: Drivable 3D Gaussian Avatars, which factor a full human avatar into layered Gaussian clusters (body, garments, face, etc.) attached to a deformable cage rig. The Gaussians move according to the cage deformation (similar to how a skeletal animation drives vertices), allowing the avatar to be animated with standard motion inputs like body joint angles or facial keypoints. Because the Gaussians carry color and volume, rendering them produces a photorealistic image of the person from the desired viewpoint, in real-time. D3GA’s system can represent complex phenomena like loose clothing (by layering Gaussian sets for clothing vs. body and sliding them) and shows higher fidelity than traditional mesh avatars on multi-view datasets. The controllability here is quite explicit: any source of motion data (motion capture, parametric face model, etc.) can be applied to the Gaussian avatar via the cage/rig. The latency of rendering is extremely low – Gaussian splatting can achieve dozens of FPS even at high resolution, since it’s basically splatting point clouds (often rasterized via GPU). In D3GA’s case, their focus is on fidelity and flexibility; they report better PSNR/SSIM than prior state-of-art on their test sequences, implying the images are very close to the ground truth video.

Another line of work focuses on audio-driven 3D Gaussian head avatars. Aneja et al. (2025) introduce GaussianSpeech, which is the first to generate photorealistic multi-view consistent talking head sequences from only audio input. They capture a person’s static head geometry as Gaussians and then train an audio-conditioned transformer that predicts how to move and recolor those Gaussians over time to lip-sync and express the speech. Importantly, GaussianSpeech’s representation supports expression-dependent color changes and high-frequency details like wrinkles: as the person speaks or smiles, the model adjusts the splats’ colors to simulate skin creases or micro-expressions. This addresses a previous limitation of NeRF-style models, which often produced waxy, expressionless skin. By using losses targeting wrinkle realism and perceptual quality, they achieve very realistic facial detail, at real-time rendering rates. Because everything is 3D, the output can be rendered from arbitrary angles (e.g., a dynamic 3D avatar for AR/VR, not just a fixed camera talking head). The trade-off is that each GaussianSpeech avatar is person-specific – one needs a multi-view capture and some training to set up each new identity (the authors even built a new large multi-view dataset of people talking to enable this research). Once set up, though, driving the avatar with new audio is very fast. GaussianSpeech achieves state-of-the-art quality among audio-driven 3D models and maintains the subject’s identity and speaking style consistently.

Latency and Performance: Neural Gaussian avatars are extremely fast at inference. Rendering thousands of 3D Gaussians can easily run in real-time on a GPU. The heavier part is the animation prediction – e.g. GaussianSpeech uses a transformer to output the motions, which must run faster than real-time (processing audio chunks perhaps in tens of milliseconds). The authors report their system produces photorealistic motion at real-time rates. Other works like GaussianTalker (2024) have similarly aimed for real-time talking head synthesis with 3DGS, with an emphasis on perfect lip-sync and stability. Because these methods don’t synthesize pixels from scratch (they render a pre-learned scene), they tend to be more efficient than diffusion models. For instance, one study comparing open-source talking-head models found Gaussian-based methods can have lower execution time than some NeRF or GAN baselines, while offering high fidelity lip synchronization.

Realism: Photorealism is a strong suit here since the avatar is literally modeled from real footage of the person. When the Gaussian representation is well-optimized, it can reproduce the person’s appearance with video-like quality. Unlike graphics models, you don’t have to artistically sculpt the face – you capture it. Thus, the subtleties of an individual’s face, hair, and even lighting in the training environment can be baked in. This yields a very convincing result for that identity. However, if the avatar is driven beyond the range of what was captured, artifacts can appear (e.g., an extreme expression or pose not seen in training might cause blobby or blurry results). Research like D3GA mitigates this by using deformation cages that extrapolate reasonably for novel poses, but fundamentally these models are not as general as generative models – each is a specialist in one person. Another consideration is multi-identity generalization: some recent works attempt generalizable Gaussian avatars (train one model across many people), but the highest fidelity so far comes from personalized models tuned per person. So the realism can be top-tier for a specific character, at the cost of needing a setup for each character.

Controllability: This approach can allow a blend of data-driven and parametric control. For example, one could drive the Gaussian avatar with a parametric face model like FLAME (getting expression coefficients from a live camera and applying those to the avatar). This would give explicit control (similar to driving a rig) but yielding photoreal output. Indeed, earlier Neural Radiance Field works (e.g. NerFace 2021) did something analogous by using a 3D morphable model to condition a NeRF. Gaussian avatars can inherit those techniques, enabling things like manual animation or retargeting of motions from one person to another by transferring rig parameters. The D3GA paper demonstrates conditioning different parts on different signals – e.g., facial expressions driven by keypoint positions (which could come from a tracking system) while body motion comes from joint angles. This modular control is harder in end-to-end generative video models. Thus, in terms of fine controllability, neural Gaussian avatars approach the flexibility of traditional graphics rigs, while maintaining much higher realism.

Cost and Setup: The obvious drawback is the need for a capture and training process per character. Capturing multi-view video and computing the Gaussian representation (which may take minutes to hours of optimization) is a barrier if one needs an avatar of any random person on the fly. There are efforts to accelerate this – e.g., using pretrained general models to reduce the needed data or optimizing from just a few images – but currently, to get the kind of quality in GaussianSpeech or D3GA, one has to invest in per-subject data. This makes the approach suitable when you want a long-term digital avatar of a specific individual (e.g. a virtual clone of a spokesperson or performer), where that up-front cost is justified. Runtime costs, however, are modest – running the animation model and rendering is often cheaper than running a giant diffusion model. This approach can run locally on a high-end PC once the avatar is built, and could potentially even run on mobile (since pure splatting is rasterization heavy but not neural heavy).

In summary, streaming Gaussian avatars offer real-time performance (60 FPS possible) and photorealism for specific individuals, plus decent control via driving signals. They require per-avatar training data and are less flexible for new identities on demand. They can be seen as an evolution of the classic idea of a “digital twin”: scan someone into a virtual, drivable copy. With Gaussian splatting and neural networks, that copy can now speak with your voice or move with your motion in real-time, with the visual fidelity of real footage.

Technical Comparison of Approaches

We now compare the three approaches side-by-side on key characteristics:

Aspect	MetaHuman/Graphics Pipeline	Generative Video (Diffusion/Transformer)	Neural Gaussian Avatar
Latency	Very low. Engine can render 60+ FPS; end-to-end latency ~30–50 ms local.	Moderate. State-of-art distilled models achieve 20–30 FPS on high-end GPUs (first-frame ~0.3–1 s). Some cloud setups hit real-time with parallel GPUs.	Very low for rendering. 3DGS rasterization easily 30–60 FPS. Animation model (if any) can be made causal to keep <100 ms delay. End-to-end often real-time.
Visual Realism	High-quality but CGI. Physically based rendering yields realistic lighting and skin, but discerning eyes may spot game-like look (depends on asset quality).	Photorealistic outputs (looks like real video). Captures subtle facial motions, blinks, etc. Slight risk of artifacts or identity drift on long runs without special handling.	Photorealistic for the captured subject (since learned from real imagery of them). Multi-view consistency and details (pores, wrinkles) achievable. Outside captured motion range, may degrade.
Controllability	Explicit & fine-grained. Animators can directly set facial expressions, body poses, etc., via rig controls. Easy to integrate with motion capture, puppetry, or hand animation.	Limited explicit control. Mostly driven by input audio/text. Hard to enforce a specific gesture at a specific time (stochastic generation). Some works add high-level controls (e.g. “smile” prompt) or use intermediate motion representations, but not as direct as rigs.	Moderate to high. If driven by parametric models (e.g. a skeleton or FLAME), can get fine control similar to rigging. Audio-driven ones give up some direct control for convenience, but one could override or blend with tracked motions. Essentially, controllability depends on how you drive the Gaussian avatar (it can be data-driven or param-driven).
Adaptability (New Identity)	Moderate effort. New character must be created (via MetaHuman Creator or scanning). No training, but manual design or capture needed. MetaHuman provides a ready library of realistic faces, but matching a specific real person’s likeness might require custom sculpting.	High/One-shot. A single photo can instantiate a new talking avatar. No retraining needed per identity – model generalizes. Just supply an image of the person (and possibly a few seconds of audio for voice cloning if needed externally).	Low (personalized). Requires a multi-view capture or at least a few images of the person and an optimization step to create the neural avatar. Not one-shot; each identity is a separate model. There is ongoing research into more general Gaussian avatars, but the best quality is from per-person training.
Training/Data	No ML training per character (the rig is generic). However, creating a highly realistic asset might involve 3D scanning or artists. The underlying technology (facial solvers, etc.) is pretrained (e.g. Epic’s solver was developed on a dataset) but user doesn’t train it.	Huge training cost for the base model (hundreds of videos, large compute). Once trained, no further training per user. Data: large talking head video corpora, often with diverse speakers for generality.	Substantial data per subject. Typically a few minutes of multi-view video or many images of the person are used to optimize the avatar. Training time can be hours per identity. The driving model (audio→motion) might be trained on a broader set for generalization, but the avatar itself is person-specific.
Runtime Requirements	A capable PC or console with a GPU. Unreal Engine or similar needed to render. Latency low enough for local use or via high-framerate streaming. Cloud not typically used due to high bandwidth needed for 60fps video streaming (but possible with Pixel Streaming).	Typically requires a GPU with ML framework. Cloud deployment common (for wider accessibility and to scale heavy models). Running locally is possible if user has a strong GPU (some optimized models can even run on consumer GPUs at lower frame rates). Memory and compute scales with model size; lighter distilled models ~a few GB VRAM.	A GPU is needed to render the splats and run any neural driving model. However, can run on a single GPU or potentially edge devices if optimized. Could be deployed locally for an application (after the initial avatar creation). Bandwidth: one can stream the 3D avatar parameters to clients and render there, or stream the video – flexible since you control the render.
Strengths	- Precise control and determinism (good for authoring or production scenarios).
- Proven real-time performance (used in live broadcasts, etc.).
- Stable output (no randomness once rigged; every run is the same given same input).
- Mature tooling (Unreal, Unity, etc.) and easy integration with interactivity (game logic, physics).	- Photoreal output from minimal input (just an image and audio).
- Fast setup for new avatars (no modeling, no per-subject training).
- Can incorporate learned behaviors (e.g. natural eye movements, listening gestures) that would be hard to hand-author.
- Rapid improvement via ML community and large data.	- Highest fidelity for specific person. Essentially a “clone” of a real person in both appearance and motion nuance.
- Real-time rendering even at high resolution (leverages graphics hardware well).
- Can be very responsive (e.g. low motion-to-photon delay) which is great for VR/AR telepresence.
- Combines well with traditional animation principles (since you can drive it with explicit signals).
Weaknesses	- Building a truly photoreal human by hand is hard; slight stylization can cause uncanny valley.
- Not as plug-and-play for everyday users (requires using a game engine or dedicated app).
- While real-time, scaling to many concurrent users is hard outside of cloud gaming setups (each client needs the rendering capability).	- Heavy compute for real-time; expensive to deploy at scale (though hardware and optimization are improving).
- Less direct controllability; model might do something unintended (e.g. tilt head) and you can’t easily stop it except by re-conditioning it.
- Some risk of artifacts (flicker, slight off-sync) if model fails, especially under unusual inputs.
- Ethical concerns: photoreal “deepfake-like” generation can be misused if not regulated.	- Requires capture/training per person (not instant).
- Less flexible in portraying something the person didn’t actually do (extrapolating far beyond training data could fail).
- Typically fixed identity (can’t morph into a totally different-looking person easily; one model = one person).
- Tooling is less mature than game engines; mostly in research code form, though improving.

Table: Comparison of near real-time digital human approaches across key factors.

In essence, graphics-based avatars excel in precision and interactivity at the cost of ultimate realism, generative models excel in realism and ease of deployment but at higher compute and with limited explicit control, and neural Gaussian avatars excel in personalized realism and performance but sacrifice the “instant-anyone” convenience.

Open-Source Implementations and Resources

Many of the advancements discussed are available as open-source projects or demos, lowering the barrier to experimentation:

MetaHuman (Unreal Engine) – While the MetaHuman assets themselves are proprietary (free for use in Unreal), Epic provides extensive documentation and sample projects for real-time MetaHuman animation. The MetaHuman Animator tools released in 2023 are included in UE5. NVIDIA Audio2Face is another free tool (though closed-source) that can plug into Unreal to drive MetaHuman faces from audio, demonstrated via plugins. For an open alternative, the community project FaceAR.js and others demonstrate using ARKit face capture in engines like Three.js. Essentially, if you have Unreal Engine, you can download a MetaHuman and use an iPhone (or webcam with software) to puppeteer it in real-time.

Diffusion/Generative Avatar Models: Several research projects have code:

SadTalker (CVPR 2023) – A popular codebase that takes a single image and an audio file to produce a talking head video with head movements. It uses a 3D motion coefficient prediction to drive a generative network, achieving good lip-sync and moderate realism. GitHub: OpenTalker/SadTalker.

Avatar Forcing (2025) – Code is anticipated (from the authors at KAIST) for the real-time interactive avatar framework (which uses diffusion forcing for fast response). The paper’s techniques might be integrated into future projects.

LiveTalk (2025) – This was a large system (audio-LLM + video diffusion) by Alibaba researchers. While the full pipeline isn’t open, components like the OmniAvatar model (1.3B audio-driven diffusion) have been released. OmniAvatar’s model weights were made available in late 2024, and a GitHub repo omni-avatar/OmniAvatar provides code for audio-driven talking-head diffusion (supporting one-shot avatars).

GeneFace++ (2023) – An improved NeRF-based talking head that is real-time capable. It combines a 3D face model with a generative renderer. Code: genefaceplusplus. This can be seen as a hybrid (described more below) but it’s open-source and can run on a single GPU for real-time audio-driven talking head generation with strong lip-sync.

First Order Model & Avatarify – Though a bit older (2019–2020), these are well-known open projects for real-time puppeteering of an image. Avatarify uses the first-order motion model (FOMM) to allow a webcam video to drive another person’s face in real-time. It’s not as realistic as diffusion or neural 3D methods, but it’s lightweight and interactive. GitHub: alievk/avatarify.

Additionally, there are “Awesome Talking Head” collections that list many papers and their code links (e.g., the repository harlanhong/awesome-talking-head-generation aggregates recent methods and whether code is available).

Gaussian Neural Avatars:

D3GA (3DV 2025) – Facebook Research released the code for D3GA, allowing one to train a drivable 3D Gaussian avatar given multi-view video data. GitHub: facebookresearch/D3GA. It includes tools for setting up the cage deformations and rendering the Gaussians, plus example datasets.

GaussianSpeech (ICCV 2025) – The authors have provided a project page and a GitHub repository shivangi-aneja/GaussianSpeech. As of early 2026, the code and dataset are expected to be released (the repo exists with the paper abstract and teaser). This will enable audio-driven 3D head generation on your own data.

VASA & GaussianTalker – There are papers like VASA-3D (2024) and GaussianTalker (2024) which also address audio-driven Gaussian splats. Their code status isn’t certain, but given the trend, at least one implementation may surface publicly. Keep an eye on paperswithcode or authors’ GitHub pages for “talking head Gaussian” for any open-source releases.

NeRF-based predecessors – Though not Gaussian, works like AD-NeRF (2022) and Neural Head Avatars (2022) have code to train a NeRF to do talking heads. These can be repurposed or serve as a starting point for those interested in neural rendering avatars. E.g., yzhou359/ADNeRF trains a NeRF that lip-syncs to audio (but it’s not real-time). They show the lineage of how Gaussian methods evolved from NeRFs.

Each approach’s open implementations have different hardware needs and complexity, so one might choose based on their use case. For instance, for a quick demo, SadTalker or Avatarify can be run with just a single image input and a standard GPU. For highest quality with custom control, diving into D3GA or GeneFace++ is more involved (and requires multi-view data), but offers a lot of flexibility. The open-source ecosystem is rapidly growing, meaning developers can experiment with combining these approaches as well.

Hybrid Strategies and Emerging Combinations

Given the strengths and weaknesses of each approach, researchers and engineers are naturally exploring hybrid solutions that combine elements of graphics, generative models, and neural renderers. These hybrids aim to achieve the best of both worlds – e.g., the controllability of a rig with the realism of a generative model. We discuss a few promising hybrid strategies:

1. MetaHuman + Generative Enhancement

One idea is to start with a MetaHuman (or similar graphics avatar) for real-time responsiveness and precise control, then use a generative model to enhance the realism of the rendered frames. For example, a MetaHuman can produce a coarse animation (with perfectly synced lips and correct head pose), and an image-to-image diffusion model could refine this output to add more lifelike skin texture, subtle expression details, or just convert the CGI look to a photoreal person. This is conceptually similar to how some style-transfer or upscaler AI filters work in post-processing, but here it would be in real-time.

A concrete research example is GeneFace (2022) and its successor GeneFace++ which essentially do this: they use a 3D face model to ensure accurate lip movement and pose, and then feed those parameters into a generative network to produce the final face video. In GeneFace, an intermediate 3D landmark predictor (driven by audio) guarantees the mouth movements align perfectly with speech, addressing the sync issue, while a neural renderer gives a realistic appearance. The result is a talking head that is both highly realistic and well-synced (something pure generative models sometimes struggled with). Another instance is Nvidia’s effort where Audio2Face drives a rig, and then the output is passed through a GAN to match the style of a target video – effectively a two-stage pipeline (first physically correct, second photoreal).

Advantages: The primary advantage is combining precision with realism. The rig can enforce hard constraints (no off-sync, no impossible expression), and the generative step adds the rich detail. This can mitigate the “uncanny valley” of purely graphics output. It also provides a fallback: if the generative part fails or lags, you still have the rig output which is intelligible, if not as pretty. In production, one could imagine a system that streams the Unreal Engine output immediately, and replaces frames with diffusion-enhanced versions a fraction of a second later once they’re ready (similar to how some videogames do real-time, then improve quality after a delay).

Challenges: This hybrid is technically complex. Ensuring the generative model faithfully follows the base animation is non-trivial; there’s a risk the AI might introduce its own unwanted movements or alter the identity. It requires training the generative model to condition on the rendered frames (or the rig parameters) as guidance. There’s also a latency concern – adding a diffusion on top will add some delay, so significant optimization is needed to keep the pipeline real-time. Some early attempts run slower than real-time, making them more suitable for high-quality rendering than truly interactive use. However, with distillation, a diffusion model conditioned on synthetic renders could possibly run fast enough.

Another challenge is temporal consistency: the generative model must not produce flickering changes frame to frame. Techniques like conditioning on past frames or using video diffusion (which inherently works on frame sequences) are employed to address this. For example, a pipeline might use the rig output as a conditioning video for a latent video diffusion model (so it “paints over” the rough video with enhanced realism).

Despite challenges, this hybrid approach is very appealing for applications like virtual anchors or entertainers, where an artist can animate a character (ensuring it does exactly what’s needed) and an AI makes it look indistinguishably real. We expect to see more work in this direction; some companies likely have internal systems doing this (rendering a CG avatar and then neural-rendering it to a target actor’s likeness for dubbing, etc., as a form of real-time deepfake with a puppeteering stage).

2. Gaussian Avatar + Parametric Driver (Neural+Physics Fusion)

Another hybrid strategy is marrying the neural 3D avatar approach with classical parametric models for control. As discussed, one can drive a Gaussian avatar with signals like keypoints or blendshape coefficients. In practice, this could look like using a standard face tracking system (like Apple ARKit’s 52 blendshape coefficients or a FLAME model fit) to capture a user’s expressions, and then applying those to a neural head avatar of another person. This way, one person in real time can puppeteer a photoreal avatar of someone else – a bit like face swap, but 3D and learned. The benefit is the low latency and stability of the parametric tracking (which runs at high FPS), combined with the high fidelity of the neural rendering. Research by Thies et al. (2019) on Neural Head Reenactment was an early form of this: they used facial landmark tracking to drive a neural rendering of another face. Now with Gaussians, the quality and speed are much improved.

One concrete recent work is VASA-3D (2024) – which stands for “Voice Activated ~ 3D Avatar”. It uses a hybrid of audio-driven and pose-driven control for a Gaussian head avatar. The system might, for example, use audio to drive the mouth and a learned model to add appropriate eye and head movements, but also allow external inputs (like a head rotation from a VR headset) to override the head pose of the avatar. This multi-driver fusion is a powerful idea: separate channels (audio, vision, manual control) can each control different aspects of the avatar. D3GA’s layering of body and face with different signals is a similar philosophy.

Advantages: The obvious advantage is versatility. A parametric model (like a skeleton or blendshapes) is interpretable and can be edited or procedurally generated, while the neural avatar ensures the output looks real. This combination can handle scenarios like: a live performer’s face is tracked to provide expressions, an AI provides the voice and lip-sync, and together they drive a celebrity’s Gaussian avatar to create a real-time hologram of that celebrity mirroring the performer’s expressions. The parametric layer ensures that even if the AI audio fails, the expressions from the performer (or some fallback) can keep the avatar alive.

It also allows applying existing animation data to neural avatars. For instance, if you have a library of facial animations (from an animation studio or a game), you could apply them to a neural avatar to create new content quickly. Essentially, neural avatars could leverage the decades of techniques in character animation by serving as the “renderer” for traditional motion data.

Challenges: A key challenge is calibration between the parametric driver and the neural avatar. The avatar must be trained or calibrated to respond correctly to the parameters. If you use FLAME, you might need to incorporate FLAME parameters into the training of the neural avatar (like NerFACE did for NeRFs). Mismatches can cause latency or inaccuracies (e.g. the avatar smile doesn’t quite match the driver’s smile). There is also the issue of generalization: if the parametric driver pushes the avatar beyond what it saw (like an extreme combination of blendshapes), will it still render correctly? Ensuring the neural representation can extrapolate or at least gracefully degrade is an ongoing research topic.

Latency is usually low in this approach (parametric tracking is maybe ~10 ms, and rendering is fast), but if the pipeline includes any learned mapping (like audio to expression), that adds some inference delay. Still, we’re likely talking tens of milliseconds – quite suitable for live interaction.

3. Generative Models + Traditional Animation Controllers

This hybrid is somewhat akin to #1 but can be considered more generally: using traditional animation controllers (rigs, curves, physics simulations) in conjunction with a learned generative module. A notable example: GLDiTalker (Lin et al. 2024) uses a graph-based 3D face model (essentially a controller with vertices and edges) and a latent diffusion transformer to generate its motion. Here the generative model doesn’t output pixels; it outputs animation (in the form of mesh vertex movements). This is a reversal of the usual “gen -> pixels” – instead it’s “gen -> motion, then render via standard means”. One could imagine extending that: a diffusion model could generate body motion (like a dance) and then a game engine character performs it. In the context of digital humans, one might have a transformer that generates a sequence of blendshape coefficients from text descriptions of emotion, which then feed into a facial rig that finally drives a digital human or even a Gaussian avatar.

By splitting the problem – content generation by AI and rendering & enforcement by deterministic controllers – we get a robust system. The AI can come up with natural motions or fill in details (like micro-expressions), while the controller ensures physical validity and offers a point of intervention for animators.

Example use-case: A virtual teacher avatar might have an AI that takes the lesson script and generates likely gestures, head nods, and facial expressions (that’s the generative part). Those are turned into animation curves (e.g. head yaw angle over time, smile intensity over time), which are then applied to a rigged avatar (MetaHuman). If something looks off, a human designer can adjust the curves (since they’re interpretable) without retraining any model. This combination of AI-driven automation and human tunability is very powerful for practical deployments.

Another instance is using physics-based controllers with generative models: for example, a diffusion model could generate a rough motion for a wave of the hand, and then an inverse kinematics controller could adjust that motion to satisfy exact hand-target constraints or avoid body penetration – merging AI creativity with rule-based refinement.

Challenges: Combining these requires bridging representation gaps. Traditional animation is often deterministic and continuous, whereas generative model outputs can be noisy or have no one-to-one correspondence to rig parameters. Techniques like motion retargeting and post-hoc optimization might be needed to map one to the other. There’s also a risk that the generative model’s contribution becomes moot if the traditional controller has to heavily correct it (for example, if the AI generates an impossible motion that the controller then replaces with a default, you’ve wasted the AI computation). Therefore, one needs to either constrain the AI (have it generate in a latent space aligned with the controller’s capabilities, as GLDiTalker does with a graph latent) or allow the controller some flexibility to follow the AI’s lead (like blending between raw AI output and a safe motion).

Despite these challenges, this hybrid is very promising for tool-driven workflows – think of an animator having an AI assistant: “Generate a surprised expression animation for 2 seconds”, the AI produces the animation curves, and the animator tweaks them. In real-time systems, a simpler version might be “AI picks an appropriate pre-built animation to play based on context, and minor variations are added by a generative module to avoid repetition”.

4. Other Hybrids and Future Directions

Beyond the three asked in the question, it’s worth noting the possibility of full integration: e.g., a system where a generative model renders the face, but a game engine handles the body. Or a NeRF/diffusion hybrid where a NeRF provides a coarse view-dependent backbone and a diffusion model fills in high-frequency details (some works have started exploring neural rendering + diffusion for static scenes – that could extend to avatars too).

Another frontier is speech-driven vs gesture-driven integration. For example, a hybrid avatar might use a diffusion model to generate realistic hand gestures from speech text (some models exist that do speech-to-gesture), while using a parametric face driven by audio for the lips, and a neural renderer for the final face. Each part (hands, body, face) could be handled by the best method suited, yet all come together in one avatar.

One more hybrid worth mentioning is style blending: using generative models to alter the style or identity of a rigged performance. For instance, take a recorded or live facial performance (from an actor or even a MetaHuman), and use a video-to-video translation model (often diffusion or GAN-based) to change the identity to a target actor’s face. This is akin to advanced deepfakes that operate on live video. It separates the motion source (the performance, which could be real or animated) and the appearance (the target person). Such a pipeline could be considered a hybrid between pure generative (since the final pixels are AI generated from one person to another’s face) and performance capture. Early examples include Disney’s research on face re-aging and swapping in video in real-time for film production. While not explicitly covered in the above categories, it’s a relevant hybrid direction for digital humans – the idea of live identity transfer combined with actor-driven animation.

Summary of Hybrid Benefits & Challenges: Hybrids aim to deliver realism, responsiveness, and control simultaneously. They often outperform pure approaches in specific metrics (e.g., better lip-sync than pure diffusion, or more realism than pure CG). However, they also inherit the complexity of both paradigms: they can be harder to build and debug, requiring expertise in graphics engines and ML. Performance tuning is also double trouble – e.g., you need both the engine and the model to be efficient to keep frame rates high. Synchronization between modules (e.g., ensuring the generative part doesn’t lag one frame behind the rig) is another engineering headache. Despite that, prototypes combining these methods are appearing in both academia and industry, because the end result – a truly convincing, fast, and controllable digital human – is so desirable.

Discussion and Outlook

The rapid progress in responsive digital human technologies in the last two years is bringing us closer to avatars that can engage in natural, face-to-face style conversations with us or represent us in virtual worlds. Each approach – traditional graphics, generative AI, neural rendering – contributes vital pieces: one gives interactivity, another gives authenticity, and another gives personalization. Rather than one replacing the others, we are likely to see convergence. For instance, future MetaHumans might internally use neural networks to render facial detail (making them as photoreal as diffusion models, but still fully controllable), or generative avatar systems might incorporate explicit 3D models to get the benefits of both.

Latency vs. Quality: A recurring theme is the trade-off between latency and quality. Real-time systems historically meant compromising visual fidelity, but that gap is closing. With clever distillation and powerful hardware, even complex models are hitting real-time thresholds. There is active work on quality–latency trade-off algorithms where the system can adjust on the fly – e.g., run at lower quality when resources are low or when lag would be unacceptable (during fast back-and-forth dialogue), then scale up when there’s more headroom. This adaptive approach will be important as these avatars move to devices like AR glasses (where you can’t always have a data-center GPU on your head, but maybe cloud can assist). Some prototypes already consider multi-turn dialogue scenarios and measure how quickly the avatar can respond in each turn – which is crucial for not feeling laggy in conversation.

Evaluation and Benchmarks: The community is developing benchmarks for these systems that consider not just video quality but interactive performance and user experience. Metrics like lip-sync error, reaction time, motion coherence over long periods, and user preference studies (does the avatar seem attentive? engaging?) are being used. Early results show that users strongly prefer avatars with responsive listening behaviors and timely nods/smiles (the kind of improvements the interactive diffusion models and Avatar Forcing are adding). So it’s not just about photorealistic speaking, but about behaving realistic in a two-way interaction. This human-centric evaluation will drive more hybrid approaches – because to get those behaviors, you might need both a language model (to know when to nod) and a generative visual model (to execute the nod naturally).

Deployment considerations: On the engineering side, developers must choose an approach based on use case. If you need a hundred virtual agents running concurrently in a virtual world, the GPU budget might favor a rigged or semi-rigged approach (possibly with some neural upscaling) because one GPU per agent for diffusion is too costly. If you need a personalized avatar for a VIP with absolute realism, capturing and training a neural avatar might be worth it. Cloud services are emerging that host these models (some startups offer “AI avatar” APIs – usually using generative models). Meanwhile, game engines are integrating more ML (Unity and Unreal both now have plugins for integrating avatar ML models). It’s plausible that in a couple of years, a creator can flip a switch in Unreal: “render this character with neural enhancement” or “auto-lip-sync with diffusion model” once these techniques mature and standardize.

Ethical and creative control: With photoreal digital humans, especially ones driven by AI, come concerns: deepfake misuse, identity theft, or just the ethical line of recreating someone who didn’t consent. Real-time deepfakes are practically here (an avatar could mimic a celebrity in a live call). The technology community is aware of this – some papers include an ethics discussion. Watermarking, gatekeeping models, or requiring explicit consent for identity cloning might become necessary norms. On a positive note, these avatars can enable powerful new forms of expression and connection (e.g., letting a user embody any persona in real time, or breaking language barriers by having your avatar speak in another language with your likeness). The discussion around these societal impacts is ongoing and will influence technical directions (for example, more focus on identity protection or manual override controls in avatar systems).

In conclusion, the near real-time digital human field is rapidly evolving. 2023 brought us diffusion models trimmed to real-time and neural rendering that can be interacted with; 2024–2025 is refining these into user-friendly systems (as evidenced by things like LiveTalk which combine multiple AI modalities). We can expect even tighter integration of language understanding, voice synthesis, and visual animation – essentially full conversational AI avatars that not only look and sound human but also act and react in a human-like loop. The fusion of methods we surveyed is paving the way for digital humans that are simultaneously realistic, responsive, and controllable, which has been a long-standing goal in computer graphics and AI. The day is not far when interacting with a digital character in real-time feels nearly as natural as a video call with a real person – and it will likely be thanks to the combination of the techniques reviewed in this article.