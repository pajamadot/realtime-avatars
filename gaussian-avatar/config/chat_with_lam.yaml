# =============================================================================
# OpenAvatarChat config — LAM Gaussian Avatar + OpenAI LLM
#
# Pipeline: SileroVAD -> SenseVoice ASR -> OpenAI LLM -> EdgeTTS -> LAM Avatar
# Rendering: client-side Gaussian splatting in browser (WebGL)
# GPU: minimal — ASR (SenseVoice) + Audio2Expression run on GPU
# Concurrent sessions: up to 5 (each uses ~0.5-1GB VRAM)
# =============================================================================

default:
  logger:
    log_level: "INFO"

  service:
    host: "0.0.0.0"
    port: 8282
    cert_file: "ssl_certs/localhost.crt"
    cert_key: "ssl_certs/localhost.key"

  chat_engine:
    model_root: "models"
    concurrent_limit: 5

    handler_search_path:
      - "src/handlers"

    handler_configs:
      # --- Client: LAM Gaussian avatar rendered in browser ---
      LamClient:
        module: client/h5_rendering_client/client_handler_lam
        asset_path: "lam_samples/barbara.zip"
        connection_ttl: 900  # max session duration (seconds)

      # --- VAD: Silero voice activity detection ---
      SileroVad:
        module: vad/silerovad/vad_handler_silero
        speaking_threshold: 0.5
        start_delay: 2048
        end_delay: 5000
        buffer_look_back: 5000
        speech_padding: 512

      # --- ASR: SenseVoice (local, runs on GPU) ---
      SenseVoice:
        enabled: true
        module: asr/sensevoice/asr_handler_sensevoice
        model_name: "iic/SenseVoiceSmall"

      # --- TTS: Edge TTS (free, no API key needed) ---
      # Switch to CosyVoice below if you want higher quality.
      EdgeTTS:
        enabled: true
        module: tts/edgetts/tts_handler_edge_tts
        voice: "en-US-AriaNeural"

      # --- TTS alternative: CosyVoice via DashScope API ---
      # Uncomment below and comment out EdgeTTS above to use.
      # CosyVoice:
      #   enabled: true
      #   module: tts/bailian_tts/tts_handler_cosyvoice_bailian
      #   voice: "longxiaocheng"
      #   model_name: "cosyvoice-v1"
      #   # api_key: ""  # reads DASHSCOPE_API_KEY from .env

      # --- LLM: OpenAI-compatible endpoint ---
      LLMOpenAICompatible:
        enabled: true
        module: llm/openai_compatible/llm_handler_openai_compatible
        model_name: "gpt-4o-mini"
        enable_video_input: false
        history_length: 20
        system_prompt: >
          You are a friendly digital human assistant having a face-to-face
          conversation. Keep your responses concise (2-3 sentences). Be warm,
          natural, and conversational. Use proper punctuation.
        api_url: "https://api.openai.com/v1"
        # api_key: ""  # reads OPENAI_API_KEY from .env

        # --- Alternative: Ollama (local, no API key) ---
        # api_url: "http://host.docker.internal:11434/v1"
        # model_name: "llama3.2"

        # --- Alternative: DashScope (Qwen) ---
        # api_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
        # model_name: "qwen-plus"
        # api_key: ""  # reads DASHSCOPE_API_KEY from .env

      # --- Avatar driver: LAM Audio2Expression ---
      LAM_Driver:
        module: avatar/lam/avatar_handler_lam_audio2expression
