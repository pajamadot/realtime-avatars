{
  "id": "avatar-apis",
  "title": "Avatar Provider APIs",
  "level": 2,
  "parentConcept": "avatar-providers",

  "explanation": {
    "level3Assumes": "You understand that avatar providers generate video from audio, but not the specific APIs and integration patterns.",
    "thisExplains": "How to integrate with major avatar providers (Hedra, Tavus, Simli, HeyGen), their API patterns, and best practices for production deployment.",
    "text": "Avatar providers abstract away the complexity of neural rendering. Each has unique capabilities and integration patterns.\n\n**Common API Pattern:**\n\n```\n1. Create Session\n2. Configure Avatar (image/video/preset)\n3. Start WebRTC Connection\n4. Stream Audio â†’ Receive Video\n5. Close Session\n```\n\n---\n\n**Hedra API:**\n\nSimplest integration - single image to avatar:\n\n```python\nfrom livekit.plugins import hedra\n\n# Create session with just an image\navatar_session = hedra.AvatarSession(\n    avatar_participant_identity=\"my-avatar\",\n    avatar_image=Image.open(\"face.png\"),\n)\n\n# Or use a pre-generated avatar ID\navatar_session = hedra.AvatarSession(\n    avatar_id=\"hedra_avatar_abc123\"\n)\n\n# Start streaming\nawait avatar_session.start(agent_session, room=livekit_room)\n```\n\n**Key Features:**\n- Single image input (no training needed)\n- Native LiveKit integration\n- <100ms latency\n- ~$0.05/minute\n\n---\n\n**Tavus API:**\n\nPersonalized digital twins from video:\n\n```python\nimport tavus\n\n# Create replica (one-time, requires 2min video)\nreplica = tavus.create_replica(\n    training_video_url=\"https://...\",\n    replica_name=\"John's Twin\"\n)\n\n# Start conversation\nconversation = tavus.Conversation.create(\n    replica_id=replica.id,\n    persona_id=\"helpful-assistant\",\n    conversational_context=\"Customer support agent\"\n)\n\n# Get WebRTC connection details\nconnection = conversation.get_connection_info()\n# Returns: { ice_servers, offer_sdp, ... }\n```\n\n**Key Features:**\n- Personalized replicas from 2-min video\n- Phoenix-3 rendering (emotional intelligence)\n- Built-in turn-taking (Sparrow-0)\n- Higher latency (~600ms) but more realistic\n\n---\n\n**Simli API:**\n\nWebRTC-native with multiple SDK options:\n\n```javascript\n// JavaScript client\nimport { SimliClient } from 'simli-client';\n\nconst simli = new SimliClient();\n\nawait simli.initialize({\n    apiKey: process.env.SIMLI_API_KEY,\n    faceId: \"face_xyz\",\n    maxSessionLength: 3600,\n});\n\n// Connect audio source\nconst audioTrack = await navigator.mediaDevices.getUserMedia({ audio: true });\nsimli.sendAudioStream(audioTrack);\n\n// Receive video\nsimli.on('videoFrame', (frame) => {\n    videoElement.srcObject = frame;\n});\n```\n\n**Key Features:**\n- Native WebRTC client libraries\n- Flutter, Python, JavaScript SDKs\n- <300ms speech-to-video latency\n- Configurable idle timeout\n\n---\n\n**HeyGen API:**\n\nEnterprise-focused with streaming SDK:\n\n```javascript\nimport HeyGenStreaming from '@heygen/streaming-sdk';\n\nconst avatar = new HeyGenStreaming({\n    apiKey: process.env.HEYGEN_API_KEY,\n    avatarId: 'avatar_preset_001',\n    quality: 'high',  // high, medium, low\n});\n\nawait avatar.connect();\n\n// Two modes:\n// TALK mode - LLM-powered responses\nawait avatar.talk(\"Hello, how can I help you?\");\n\n// REPEAT mode - direct playback\nawait avatar.repeat(audioBuffer);\n```\n\n**Key Features:**\n- Pre-built avatar library\n- Custom avatar training\n- Streaming + batch API\n- LiveKit integration option\n\n---\n\n**Integration Best Practices:**\n\n**1. Session Management:**\n```python\n# Always clean up sessions\ntry:\n    session = await create_session()\n    # ... use session\nfinally:\n    await session.close()\n```\n\n**2. Error Handling:**\n```python\ntry:\n    await avatar.send_audio(chunk)\nexcept RateLimitError:\n    await asyncio.sleep(1)\n    await avatar.send_audio(chunk)\nexcept ConnectionError:\n    await avatar.reconnect()\n```\n\n**3. Latency Monitoring:**\n```python\nstart = time.time()\nawait avatar.send_audio(audio)\nvideo = await avatar.receive_video()\nlatency = time.time() - start\nmetrics.record('avatar_latency', latency)\n```\n\n**4. Graceful Degradation:**\n```python\ntry:\n    video = await avatar.generate(audio, timeout=2.0)\nexcept TimeoutError:\n    # Fall back to static image or simpler avatar\n    video = static_fallback_image\n```\n\n**Comparison Matrix:**\n\n| Feature | Hedra | Tavus | Simli | HeyGen |\n|---------|-------|-------|-------|--------|\n| Input | 1 image | 2min video | 1 image | Preset/custom |\n| Latency | <100ms | ~600ms | <300ms | Variable |\n| LiveKit Native | Yes | No | No | Optional |\n| Personalization | Low | High | Medium | Medium |\n| Pricing | $/min | $/min | $/min | Tiered |"
  },

  "visual": {
    "type": "interactive",
    "demoId": "provider-comparison",
    "caption": "Compare avatar providers side-by-side: same audio, different providers, measure latency and quality."
  },

  "prerequisites": [
    "webrtc",
    "rest-apis"
  ],

  "insight": "Avatar APIs are like video conferencing with an AI participant - you send audio, they send back a talking face, all in real-time over WebRTC."
}
