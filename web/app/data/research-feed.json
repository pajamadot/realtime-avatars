{
  "updatedAt": "2026-02-11T10:57:47.506Z",
  "source": "arXiv",
  "methods": {
    "metahuman": {
      "label": "MetaHuman / Graphics Pipeline",
      "query": "cat:cs.CV AND (all:metahuman OR (all:realtime AND all:facial AND all:animation) OR (all:facial AND all:retargeting AND all:rig) OR all:blendshape)",
      "items": [
        {
          "title": "RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data",
          "link": "https://arxiv.org/abs/2511.18601v1",
          "published": "2025-11-23T19:55:08.000Z",
          "authors": "Wenchao Ma",
          "summary": "In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive...",
          "tags": []
        },
        {
          "title": "Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation",
          "link": "https://arxiv.org/abs/2510.25234v1",
          "published": "2025-10-29T07:29:21.000Z",
          "authors": "Yuxiang Mao",
          "summary": "Expressions are fundamental to conveying human emotions. With the rapid advancement of AI-generated content (AIGC), realistic and expressive 3D facial animation has become increasingly crucial. Despite recent progress in speech-driven lip-sync for talking-face animation, gener...",
          "tags": [
            "audio-driven",
            "lip-sync"
          ]
        },
        {
          "title": "ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion",
          "link": "https://arxiv.org/abs/2510.04706v1",
          "published": "2025-10-06T11:20:56.000Z",
          "authors": "Foivos Paraperas Papantoniou",
          "summary": "Human-centric generative models designed for AI-driven storytelling must bring together two core capabilities: identity consistency and precise control over human performance. While recent diffusion-based approaches have made significant progress in maintaining facial identity...",
          "tags": [
            "diffusion"
          ]
        },
        {
          "title": "Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark",
          "link": "https://arxiv.org/abs/2508.12438v1",
          "published": "2025-08-17T17:10:13.000Z",
          "authors": "Yaron Aloni",
          "summary": "Dynamic facial expression generation from natural language is a crucial task in Computer Graphics, with applications in Animation, Virtual Avatars, and Human-Computer Interaction. However, current generative models suffer from datasets that are either speech-driven or limited ...",
          "tags": [
            "audio-driven"
          ]
        },
        {
          "title": "Improving Facial Rig Semantics for Tracking and Retargeting",
          "link": "https://arxiv.org/abs/2508.08429v1",
          "published": "2025-08-11T19:39:04.000Z",
          "authors": "Dalton Omens",
          "summary": "In this paper, we consider retargeting a tracked facial performance to either another person or to a virtual character in a game or virtual reality (VR) environment. We remove the difficulties associated with identifying and retargeting the semantics of one rig framework to an...",
          "tags": []
        },
        {
          "title": "MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics",
          "link": "https://arxiv.org/abs/2508.04687v1",
          "published": "2025-08-06T17:50:01.000Z",
          "authors": "Ye Pan",
          "summary": "Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with multiple machine learning models, we present both non-real time and real time soluti...",
          "tags": [
            "diffusion",
            "real-time"
          ]
        },
        {
          "title": "JOLT3D: Joint Learning of Talking Heads and 3DMM Parameters with Application to Lip-Sync",
          "link": "https://arxiv.org/abs/2507.20452v1",
          "published": "2025-07-28T01:00:59.000Z",
          "authors": "Sungjoon Park",
          "summary": "In this work, we revisit the effectiveness of 3DMM for talking head synthesis by jointly learning a 3D face reconstruction model and a talking head synthesis model. This enables us to obtain a FACS-based blendshape representation of facial expressions that is optimized for tal...",
          "tags": [
            "lip-sync"
          ]
        },
        {
          "title": "CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits",
          "link": "https://arxiv.org/abs/2507.17327v1",
          "published": "2025-07-23T08:52:48.000Z",
          "authors": "Chao He",
          "summary": "With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language...",
          "tags": [
            "real-time"
          ]
        },
        {
          "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation",
          "link": "https://arxiv.org/abs/2507.12001v1",
          "published": "2025-07-16T07:56:25.000Z",
          "authors": "Hao Li",
          "summary": "While 3D facial animation has made impressive progress, challenges still exist in realizing fine-grained stylized 3D facial expression manipulation due to the lack of appropriate datasets. In this paper, we introduce the AUBlendSet, a 3D facial dataset based on AU-Blendshape r...",
          "tags": []
        },
        {
          "title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding",
          "link": "https://arxiv.org/abs/2507.06071v4",
          "published": "2025-07-08T15:14:27.000Z",
          "authors": "Chang Liu",
          "summary": "Audio-driven emotional 3D facial animation aims to generate synchronized lip movements and vivid facial expressions. However, most existing approaches focus on static and predefined emotion labels, limiting their diversity and naturalness. To address these challenges, we propo...",
          "tags": [
            "audio-driven"
          ]
        },
        {
          "title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning",
          "link": "https://arxiv.org/abs/2505.22416v1",
          "published": "2025-05-28T14:43:43.000Z",
          "authors": "Sihun Cha",
          "summary": "Accurately retargeting facial expressions to a face mesh while enabling manipulation is a key challenge in facial animation retargeting. Recent deep-learning methods address this by encoding facial expressions into a global latent code, but they often fail to capture fine-grai...",
          "tags": []
        },
        {
          "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars",
          "link": "https://arxiv.org/abs/2505.10072v3",
          "published": "2025-05-15T08:16:12.000Z",
          "authors": "Rui-Yang Ju",
          "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based method, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D h...",
          "tags": [
            "gaussian",
            "monocular",
            "real-time"
          ]
        }
      ]
    },
    "generative": {
      "label": "Generative Video Models",
      "query": "cat:cs.CV AND ((all:\"talking head\" OR all:\"portrait animation\" OR all:\"head avatar\" OR all:reenactment) AND (all:audio OR all:speech OR all:lip OR all:avatar))",
      "items": [
        {
          "title": "Toward Fine-Grained Facial Control in 3D Talking Head Generation",
          "link": "https://arxiv.org/abs/2602.09736v1",
          "published": "2026-02-10T12:49:50.000Z",
          "authors": "Shaoyang Xie",
          "summary": "Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significan...",
          "tags": [
            "audio-driven",
            "gaussian",
            "real-time"
          ]
        },
        {
          "title": "AUHead: Realistic Emotional Talking Head Generation via Action Units Control",
          "link": "https://arxiv.org/abs/2602.09534v1",
          "published": "2026-02-10T08:45:51.000Z",
          "authors": "Jiayi Lyu",
          "summary": "Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage...",
          "tags": []
        },
        {
          "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads",
          "link": "https://arxiv.org/abs/2602.07449v2",
          "published": "2026-02-07T08:58:16.000Z",
          "authors": "Tan Yu",
          "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compro...",
          "tags": [
            "audio-driven",
            "real-time"
          ]
        },
        {
          "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
          "link": "https://arxiv.org/abs/2601.22127v1",
          "published": "2026-01-29T18:49:27.000Z",
          "authors": "John Flynn",
          "summary": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and acc...",
          "tags": [
            "audio-driven",
            "diffusion",
            "transformer"
          ]
        },
        {
          "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting",
          "link": "https://arxiv.org/abs/2601.18633v1",
          "published": "2026-01-26T16:06:57.000Z",
          "authors": "Tong Shi",
          "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talki...",
          "tags": [
            "gaussian"
          ]
        },
        {
          "title": "SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video",
          "link": "https://arxiv.org/abs/2601.18851v1",
          "published": "2026-01-26T14:26:16.000Z",
          "authors": "Wei Liang",
          "summary": "Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3D...",
          "tags": [
            "monocular",
            "real-time"
          ]
        },
        {
          "title": "Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation",
          "link": "https://arxiv.org/abs/2601.12876v1",
          "published": "2026-01-19T09:31:24.000Z",
          "authors": "Zhenxuan Lu",
          "summary": "Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the ...",
          "tags": [
            "lip-sync"
          ]
        },
        {
          "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
          "link": "https://arxiv.org/abs/2601.01847v1",
          "published": "2026-01-05T07:19:38.000Z",
          "authors": "Chuhang Ma",
          "summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate...",
          "tags": [
            "audio-driven",
            "gaussian"
          ]
        },
        {
          "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement",
          "link": "https://arxiv.org/abs/2601.01749v1",
          "published": "2026-01-05T02:59:49.000Z",
          "authors": "Lei Zhu",
          "summary": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Ex...",
          "tags": [
            "audio-driven"
          ]
        },
        {
          "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
          "link": "https://arxiv.org/abs/2601.00664v1",
          "published": "2026-01-02T11:58:48.000Z",
          "authors": "Taekyung Ki",
          "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. W...",
          "tags": [
            "real-time"
          ]
        },
        {
          "title": "PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment",
          "link": "https://arxiv.org/abs/2512.22602v1",
          "published": "2025-12-27T14:14:14.000Z",
          "authors": "Bin Wang",
          "summary": "Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individu...",
          "tags": [
            "audio-driven",
            "lip-sync"
          ]
        },
        {
          "title": "SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild",
          "link": "https://arxiv.org/abs/2512.21736v3",
          "published": "2025-12-25T16:49:40.000Z",
          "authors": "Xindi Zhang",
          "summary": "High-quality AI-powered video dubbing demands precise audio-lip synchronization, high-fidelity visual generation, and faithful preservation of identity and background. Most existing methods rely on a mask-based training strategy, where the mouth region is masked in talking-hea...",
          "tags": [
            "lip-sync"
          ]
        }
      ]
    },
    "gaussian": {
      "label": "Gaussian Splatting (3DGS)",
      "query": "cat:cs.CV AND (all:gaussian AND (all:splatting OR all:\"gaussian splatting\" OR all:3dgs) AND (all:avatar OR all:human OR all:head OR all:talking OR all:conversation OR all:\"large avatar\"))",
      "items": [
        {
          "title": "Toward Fine-Grained Facial Control in 3D Talking Head Generation",
          "link": "https://arxiv.org/abs/2602.09736v1",
          "published": "2026-02-10T12:49:50.000Z",
          "authors": "Shaoyang Xie",
          "summary": "Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significan...",
          "tags": [
            "audio-driven",
            "gaussian",
            "real-time"
          ]
        },
        {
          "title": "Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering",
          "link": "https://arxiv.org/abs/2602.06343v1",
          "published": "2026-02-06T03:14:37.000Z",
          "authors": "Weiquan Wang",
          "summary": "High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposin...",
          "tags": [
            "gaussian",
            "monocular"
          ]
        },
        {
          "title": "From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors",
          "link": "https://arxiv.org/abs/2602.06122v1",
          "published": "2026-02-05T19:00:50.000Z",
          "authors": "Ding-Jiun Huang",
          "summary": "Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing lo...",
          "tags": []
        },
        {
          "title": "PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction",
          "link": "https://arxiv.org/abs/2602.05190v1",
          "published": "2026-02-05T01:34:52.000Z",
          "authors": "Ju Shen",
          "summary": "We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is p...",
          "tags": [
            "gaussian"
          ]
        },
        {
          "title": "JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction",
          "link": "https://arxiv.org/abs/2602.04317v1",
          "published": "2026-02-04T08:33:51.000Z",
          "authors": "Zihan Lou",
          "summary": "Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting...",
          "tags": [
            "gaussian",
            "monocular"
          ]
        },
        {
          "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
          "link": "https://arxiv.org/abs/2602.01674v1",
          "published": "2026-02-02T05:42:40.000Z",
          "authors": "Hail Song",
          "summary": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR ...",
          "tags": [
            "gaussian",
            "real-time"
          ]
        },
        {
          "title": "Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference",
          "link": "https://arxiv.org/abs/2601.21269v1",
          "published": "2026-01-29T05:03:29.000Z",
          "authors": "Jianglong Li",
          "summary": "The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge. Traditional 2D video compression techniques fail to preserve fine-grained ge...",
          "tags": [
            "compression",
            "diffusion",
            "telepresence"
          ]
        },
        {
          "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting",
          "link": "https://arxiv.org/abs/2601.18633v1",
          "published": "2026-01-26T16:06:57.000Z",
          "authors": "Tong Shi",
          "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talki...",
          "tags": [
            "gaussian"
          ]
        },
        {
          "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
          "link": "https://arxiv.org/abs/2601.15951v1",
          "published": "2026-01-22T13:39:29.000Z",
          "authors": "Sheng Miao",
          "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches ach...",
          "tags": [
            "gaussian"
          ]
        },
        {
          "title": "ICo3D: An Interactive Conversational 3D Virtual Human",
          "link": "https://arxiv.org/abs/2601.13148v1",
          "published": "2026-01-19T15:30:08.000Z",
          "authors": "Richard Shaw",
          "summary": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, bot...",
          "tags": [
            "multi-view"
          ]
        },
        {
          "title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation",
          "link": "https://arxiv.org/abs/2601.10606v1",
          "published": "2026-01-15T17:23:19.000Z",
          "authors": "Peng Chen",
          "summary": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while l...",
          "tags": []
        },
        {
          "title": "Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization",
          "link": "https://arxiv.org/abs/2601.07518v1",
          "published": "2026-01-12T13:17:41.000Z",
          "authors": "Fangyu Lin",
          "summary": "Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for...",
          "tags": [
            "gaussian",
            "monocular",
            "telepresence"
          ]
        }
      ]
    },
    "streaming": {
      "label": "Video Generation / Infrastructure",
      "query": "(cat:cs.NI OR cat:cs.CV) AND (all:webrtc AND (all:streaming OR all:low-latency OR all:conference OR all:sfu OR all:telepresence OR all:avatar))",
      "items": [
        {
          "title": "Lightweight Call Signaling and Peer-to-Peer Control of WebRTC Video Conferencing",
          "link": "https://arxiv.org/abs/2602.08975v1",
          "published": "2026-02-09T18:16:37.000Z",
          "authors": "Kundan Singh",
          "summary": "We present the software architecture and implementation of our web-based multiparty video conference application. It does not use a media server. For call signaling, it either piggybacks on existing push notifications via a lightweight notification server, or utilizes email me...",
          "tags": [
            "telepresence",
            "webrtc"
          ]
        },
        {
          "title": "Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization",
          "link": "https://arxiv.org/abs/2601.07518v1",
          "published": "2026-01-12T13:17:41.000Z",
          "authors": "Fangyu Lin",
          "summary": "Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for...",
          "tags": [
            "gaussian",
            "monocular",
            "telepresence"
          ]
        },
        {
          "title": "Adaptive Receiver-Side Scheduling for Smooth Interactive Delivery",
          "link": "https://arxiv.org/abs/2511.16902v1",
          "published": "2025-11-21T02:32:33.000Z",
          "authors": "Michael Luby",
          "summary": "Interactive applications such as cloud gaming, XR streaming, and real-time inference depend on data objects arriving at a steady cadence. In practice, network delay variation and recovery dynamics at the receiver distort this cadence even when transports deliver all packets co...",
          "tags": [
            "real-time"
          ]
        },
        {
          "title": "Solving the Problem of Poor Internet Connectivity in Dhaka: Innovative Solutions Using Advanced WebRTC and Adaptive Streaming Technologies",
          "link": "https://arxiv.org/abs/2506.17343v1",
          "published": "2025-06-19T14:42:01.000Z",
          "authors": "Pavel Malinovskiy",
          "summary": "Dhaka, Bangladesh, one of the world's most densely populated cities, faces severe challenges in maintaining reliable, high-speed internet connectivity. This paper presents an innovative framework that addresses poor mobile data connections through the integration of advanced W...",
          "tags": [
            "webrtc"
          ]
        },
        {
          "title": "Streaming Remote rendering services: Comparison of QUIC-based and WebRTC Protocols",
          "link": "https://arxiv.org/abs/2505.22132v1",
          "published": "2025-05-28T08:57:44.000Z",
          "authors": "Daniel Mej√≠as",
          "summary": "The proliferation of Extended Reality (XR) applications, requiring high-quality, low-latency media streaming, has driven the demand for efficient remote rendering solutions. This paper focuses on holographic conferencing in virtual environments and their required uplink and do...",
          "tags": [
            "real-time",
            "telepresence",
            "webrtc"
          ]
        },
        {
          "title": "Scalable Video Conferencing Using SDN Principles",
          "link": "https://arxiv.org/abs/2503.11649v1",
          "published": "2025-03-14T17:59:38.000Z",
          "authors": "Oliver Michel",
          "summary": "Video-conferencing applications face an unwavering surge in traffic, stressing their underlying infrastructure in unprecedented ways. This paper rethinks the key building block for conferencing infrastructures -- selective forwarding units (SFUs). SFUs relay and adapt media st...",
          "tags": [
            "telepresence",
            "webrtc"
          ]
        },
        {
          "title": "SQP: Congestion Control for Low-Latency Interactive Video Streaming",
          "link": "https://arxiv.org/abs/2207.11857v1",
          "published": "2022-07-25T00:37:19.000Z",
          "authors": "Devdeep Ray",
          "summary": "This paper presents the design and evaluation of SQP, a congestion control algorithm (CCA) for interactive video streaming applications that need to stream high-bitrate compressed video with very low end-to-end frame delay (eg. AR streaming, cloud gaming). SQP uses frame-coupl...",
          "tags": [
            "compression",
            "real-time"
          ]
        },
        {
          "title": "SnoW: Serverless n-Party calls over WebRTC",
          "link": "https://arxiv.org/abs/2206.12762v1",
          "published": "2022-06-26T01:51:30.000Z",
          "authors": "Thomas Sandholm",
          "summary": "We present a novel WebRTC communication system capable of hosting multi-party audio and video conferencing sessions without a media server. We implement various communication models based on the needs and capabilities of the communicating parties, and show that we can construc...",
          "tags": [
            "telepresence",
            "webrtc"
          ]
        }
      ]
    }
  }
}
